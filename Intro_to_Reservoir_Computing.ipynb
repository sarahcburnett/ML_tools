{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Intro_to_Reservoir_Computing.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dbp4IVUN5t9F",
        "colab_type": "text"
      },
      "source": [
        "# Let's do some reservoir computing!!\n",
        "\n",
        "\n",
        "---\n",
        "Sarah Burnett (2020) commented and modified code Sarthak Chandra gave permission to share. The techniques for reservoir computing here are explained well in the [[2017 paper](https://arxiv.org/abs/1710.07313)] by Jaideep Pathak, et al. \n",
        "\n",
        "##Normalize your data\n",
        "\n",
        "To begin, choose a collection of the data that you want to predict. \n",
        "In the example we will make some synthetic data and train on 80% of it.\n",
        "\n",
        "Let's produce the Lorenz-63' data set. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qICTXqWmgMZW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#This was taken from Wikipedia\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.integrate import odeint\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "rho = 28.0\n",
        "sigma = 10.0\n",
        "beta = 8.0 / 3.0\n",
        "\n",
        "def f(state, t):\n",
        "    x, y, z = state  # Unpack the state vector\n",
        "    return sigma * (y - x), x * (rho - z) - y, x * y - beta * z  # Derivatives\n",
        "\n",
        "state0 = [1.0, 1.0, 1.0]\n",
        "t = np.arange(0.0, 200.0, 0.05)\n",
        "\n",
        "states = odeint(f, state0, t)\n",
        "\n",
        "fig = plt.figure()\n",
        "ax = fig.gca(projection='3d')\n",
        "ax.plot(states[:, 0], states[:, 1], states[:, 2])\n",
        "plt.draw()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzFJF6Kn5seJ",
        "colab_type": "text"
      },
      "source": [
        "It's like a butterfly! Lorenz '63 is a simplified model for convective rolls in the atomosphere. Its know for its irregular behavior. \n",
        "\n",
        "Select the post-transient. (Check that it's on the attractor/butterfly). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SgP9M-WxJm8P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "states = states[500:,:]\n",
        "fig = plt.figure()\n",
        "ax = fig.gca(projection='3d')\n",
        "ax.plot(states[:, 0], states[:, 1], states[:, 2])\n",
        "plt.draw()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wQvGbaWLLCc",
        "colab_type": "text"
      },
      "source": [
        "Normalize the states. Make some noisy data and a truth set.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3EMHOrwXLh8-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pylab as pl\n",
        "\n",
        "#normalized data\n",
        "xs=pl.copy(states)\n",
        "xavg=pl.average(xs,axis=0)\n",
        "xstd=pl.std(xs,axis=0)\n",
        "xnlist=(xs-xavg)/xstd\n",
        "\n",
        "#save noiseless as the truth\n",
        "xnlistnoiseless=pl.copy(xnlist)\n",
        "\n",
        "#add noise\n",
        "xnlist=xnlist+pl.normal(0,0.015,xnlist.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOmEdvz_MDbx",
        "colab_type": "text"
      },
      "source": [
        "##Meet the Reservoir Map\n",
        "\n",
        "Below is a list of functions used for reservoir computing. The first function listed is the reservoir map\n",
        "\n",
        "$$\\mathbf{r}(t+ \\Delta t) = \\tanh \\left[ \\mathbf{A} \\mathbf{r} (t) + \\mathbf{W}_{in}  \\mathbf{W}_{out}(\\mathbf{r}(t),\\mathbf{P}) \\right]. $$\n",
        "\n",
        "with the python function below it includes a leakage parameter. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unhJpXC8MDBA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def reservoirmap(rn,inp,A,Win,leakage=0.):\n",
        "    return leakage*rn + (1.-leakage)*pl.tanh(dot(Win,inp) + dot(A,rn))\n",
        "\n",
        "def dot(a,b):\n",
        "    return a*b"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uEmWAg6xXTFC",
        "colab_type": "text"
      },
      "source": [
        "Define the leakage here. You can think of it as the regulating the amount of linear change in the reservoir. When L = 0, we have no leakage, the reservoir is capable of making a large adjustment. When L = 1, there is all leakage and no change in the reservoir. It becauses a linear update with\n",
        "\n",
        "$$\\mathbf{v}(t+\\Delta t)=\\mathbf{W}_{o u t}(\\mathbf{r}(t+\\Delta t), \\mathbf{P}).$$\n",
        "\n",
        "now just\n",
        "\n",
        "$$\\mathbf{v}(t+\\Delta t)=\\mathbf{W}_{o u t}(\\mathbf{W}_{in}\\mathbf{u}(t), \\mathbf{P})$$.\n",
        "\n",
        "Choose leakage below. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_CsohkmcXP_6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "leakage =  0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMwFEe9TZHxG",
        "colab_type": "text"
      },
      "source": [
        "##Prep your reservoir\n",
        "\n",
        "Let's meet some parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ROzJ3DZjZvzS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Dr=2000 #number of reservoir nodes\n",
        "avd=3 #3 or 5 #Adjacency matrix param, average degree of the reservoir\n",
        "(L,N)=states.shape\n",
        "training_transient = 100 \n",
        "training_length=int(L*.8)-training_transient\n",
        "testing_length=L-training_length-training_transient\n",
        "r_tr_step=min(10000,training_length) #reservoir training step for running mean, make it smaller to use less memory instead of computation time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dARWLPWbapK",
        "colab_type": "text"
      },
      "source": [
        "These parameters don't need to be tuned and changed. We like them to remain small so that the computational cost is low. Change Dr and avd if you don't get improvements tuning leakage, rho, and the training period.\n",
        "\n",
        "The following is a function to create a sparse random matrix with a particular size and average degree of freedom."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t0qa4IN6caZ8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import scipy.sparse as sparse\n",
        "from scipy.sparse import linalg as lalg\n",
        "from scipy.sparse.linalg import ArpackNoConvergence\n",
        "\n",
        "def makeAdjMatrix(Dr,avd):\n",
        "    try: \n",
        "        Ao=sparse.lil_matrix((Dr,Dr))\n",
        "        for _ in range(int(avd*Dr)):\n",
        "            Ao[pl.randint(0,Dr),pl.randint(0,Dr)]=pl.uniform(-1,1)  \n",
        "        lambda0=abs(lalg.eigs(Ao,k=1,which='LM',return_eigenvectors=False)[0])\n",
        "    except ArpackNoConvergence as err: #Don't worry about this\n",
        "        try: \n",
        "            Ao=sparse.lil_matrix((Dr,Dr))\n",
        "            for _ in range(int(avd*Dr)):\n",
        "                Ao[pl.randint(0,Dr),pl.randint(0,Dr)]=pl.uniform(-1,1)  \n",
        "            lambda0=abs(lalg.eigs(Ao,k=1,which='LM',return_eigenvectors=False)[0])\n",
        "        except ArpackNoConvergence as err:\n",
        "            if verbose:\n",
        "                print( \"trying again\")\n",
        "            Ao=sparse.lil_matrix((Dr,Dr))\n",
        "            for _ in range(int(avd*Dr)):\n",
        "                Ao[pl.randint(0,Dr),pl.randint(0,Dr)]=pl.uniform(-1,1)  \n",
        "            lambda0=abs(lalg.eigs(Ao,k=1,which='LM',return_eigenvectors=False)[0])    \n",
        "    return Ao,lambda0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9mvLkPBze2BA",
        "colab_type": "text"
      },
      "source": [
        "This rho parameter makes the system have more drastic nonlinear updates to the reservoir when receiving new information by modifying the largest eigenvalue of the initialized adjaceny matrix, A. Choose a value start close to 1. (rho = 1 means no scaling). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DTzWQlrBe523",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rho = 0.9"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Ubk-5OqnN9_",
        "colab_type": "text"
      },
      "source": [
        "Initialize the adjacency matrix, $\\mathbf{A}$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TraT0Yr6e1dC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Ao,lambda0=makeAdjMatrix(Dr,avd)\n",
        "A=Ao*rho/lambda0 \n",
        "A=A.tocsr() "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q5Kn6mM8sgus",
        "colab_type": "text"
      },
      "source": [
        "Initialize the input and output weights, $\\mathbf{W}_{in}$. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pBDnuYgwdGA4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Win_sigma=1 #range for uniform distribution, optimize this around 1\n",
        "Dinp=N\n",
        "\n",
        "def makeWin(Dr,Dinp,Win_sigma,verbose=True):\n",
        "    Win=sparse.lil_matrix((Dr,Dinp))\n",
        "    for i in range(Dr):\n",
        "        Win[i,pl.randint(0,Dinp)]=pl.uniform(-Win_sigma,Win_sigma)\n",
        "    return Win\n",
        "\n",
        "Win=makeWin(Dr,Dinp,Win_sigma)\n",
        "Win=Win.tocsr()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WwfsVtDX0QtP",
        "colab_type": "text"
      },
      "source": [
        "##Time to train the reservoir!\n",
        "\n",
        "In this stage, we aim to train our reservoir on a subset of the data that we will call the `measurements', $\\mathbf{u}(t)$. These measurements are mapped into the reservoir state space using the $D_r \\times D$ matrix $\\mathbf{W}_{in}$, where $D_r$ is reservoir dimensions and $D$ is the dimension of the input, 3 for Lorenz '63. Likewise, outputs from the reservoir are mapped back to the same state space as the inputs using the function $\\mathbf{W}_{out}(\\mathbf{r},\\mathbf{P})$, where $\\mathbf{r}(t)$ is the reservoir state and $\\mathbf{P}$ is a regularization matrix. \n",
        "\n",
        "The reservoir is updated/trained by iterating through the measurements and combining them with the reservoir states, giving the new reservoir state and output\n",
        "\n",
        "\\begin{align}\n",
        "\\mathbf{r}(t+ \\Delta t) &= \\tanh \\left[ \\mathbf{A} \\mathbf{r} (t) + \\mathbf{W}_{in} \\mathbf{u}(t) \\right], \\\\\n",
        "\\mathbf{v}(t+ \\Delta t) &= \\mathbf{W}_{out}(\\mathbf{r}(t+ \\Delta t),\\mathbf{P}).\n",
        "\\end{align}\n",
        "\n",
        "The desired effect we want the reservior to have is that the output $\\mathbf{v}(t+ \\Delta t)$ be the same as the data, $\\mathbf{v}_d(t+ \\Delta t)$. We do this by choosing $\\mathbf{P}$ such that during the training period of $-T \\leq t \\leq 0$ the following equation\n",
        "\n",
        "\\begin{equation}\n",
        "\\sum_{-T \\leq t \\leq 0} \\| \\mathbf{W}_{out}(\\mathbf{r},\\mathbf{P}) - \\mathbf{v}_d(t+ \\Delta t) \\|^2 + \\beta \\|\\mathbf{P} \\|^2\n",
        "\\end{equation}\n",
        "\n",
        "is minimized, where $\\beta >0$ is a regularization constant. Considering $\\mathbf{W}_{out}$ is taken linearly in $\\mathbf{P}$, this becomes a linear regression problem and standard approaches as mentioned before can be used to handle that. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4y0qXj4PXGXK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import scipy.sparse as sparse\n",
        "from scipy.sparse import linalg as lalg\n",
        "from scipy.sparse.linalg import ArpackNoConvergence\n",
        "import sys\n",
        "\n",
        "if sys.version_info[0]==3:\n",
        "\txrange=range\n",
        "\n",
        "def train_reservoir(xnlist,training_transient,training_length,A,Win,leakage,r_tr_step,verbose=True):\n",
        "    Dr=A.shape[0]\n",
        "    rn=pl.zeros((Dr))\n",
        "    reservoir_state_training=pl.zeros((r_tr_step,Dr))\n",
        "    RTRt=pl.outer(rn,rn)\n",
        "    RTxt=pl.outer(pl.zeros(Dr),xnlist[0])  #xnlist[0] will go in the sum twice, but doesn't matter because here i've multiplied it with a zeros vector\n",
        "    for n in xrange(training_transient):\n",
        "        rn=reservoirmap(rn,xnlist[n],A,Win,leakage)\n",
        "    for n in xrange(training_length//r_tr_step):\n",
        "        for m in range(r_tr_step):\n",
        "            reservoir_state_training[m]=sqeven(rn) #squaring the value of the reservoir state at alternate notes\n",
        "            rn=reservoirmap(rn,xnlist[r_tr_step*n+m+training_transient],A,Win,leakage)\n",
        "        RTRt = RTRt + pl.dot(reservoir_state_training.transpose(),reservoir_state_training)\n",
        "        RTxt = RTxt + pl.dot(reservoir_state_training.transpose(),xnlist[training_transient+r_tr_step*n:training_transient+r_tr_step*(n+1)])\n",
        "    return rn,RTRt,RTxt,reservoir_state_training\n",
        "   \n",
        "def reservoir_prediction(rn,Win,A,Wout,leakage,testing_length):\n",
        "    Dinp=Wout.shape[1]\n",
        "    predicted_output=pl.zeros((testing_length,Dinp))\n",
        "    for n in xrange(testing_length):\n",
        "        xn_pred=pl.dot(sqeven(rn),Wout)\n",
        "        predicted_output[n]=xn_pred\n",
        "        rn=reservoirmap(rn,xn_pred,A,Win,leakage)    \n",
        "    return predicted_output,rn\n",
        "    \n",
        "def sqeven(x):\n",
        "\tx[::2]=x[::2]**2.\n",
        "\treturn x\n",
        "\n",
        "#Train A\n",
        "rn,RTRt,RTxt,reservoir_state_training=train_reservoir(xnlist,training_transient,training_length,A,Win,leakage,r_tr_step)\n",
        "\n",
        "#Solve for Wout\n",
        "Wout=pl.solve((RTRt+beta*pl.eye(Dr)),RTxt)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J82S5YCFFIG5",
        "colab_type": "text"
      },
      "source": [
        "##Run the reservoir for prediction\n",
        "\n",
        "The next stage is the prediction phase. The approach [[Jaeger 2004](http://www.columbia.edu/cu/biology/courses/w4070/Reading_List_Yuste/haas_04.pdf)] used is to take the desired output to be equal to the input, $\\mathbf{v}_d(t+ \\Delta t) = \\mathbf{u}(t+ \\Delta t)$. Now at the time of prediction, starting at $t=0$, the reservoir states are computed by\n",
        "\\begin{equation}\n",
        "\\mathbf{r}(t+ \\Delta t) = \\tanh \\left[ \\mathbf{A} \\mathbf{r} (t) + \\mathbf{W}_{in}  \\mathbf{W}_{out}(\\mathbf{r}(t),\\mathbf{P}) \\right]. \n",
        "\\end{equation}\n",
        "With this, $\\mathbf{v}(t) = \\mathbf{W}_{out}(\\mathbf{r}(t),\\mathbf{P})$ functions as the input measurements, $\\mathbf{u}(t)$, in the training stage, except now they act as `predictions' for these values. Eventually, these predictions diverge from the truth as small errors are amplified by chaos. In the experiments in the next section, I measure the length of time the reservoir predicts accurately as the time before the error between the predictions and truth grows beyond a threshold."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2vVRoGnUFkEl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from numpy import linalg as LA\n",
        "   \n",
        "def reservoir_prediction(rn,Win,A,Wout,leakage,testing_length):\n",
        "    Dinp=Wout.shape[1]\n",
        "    predicted_output=pl.zeros((testing_length,Dinp))\n",
        "    for n in xrange(testing_length):\n",
        "        xn_pred=pl.dot(sqeven(rn),Wout)\n",
        "        predicted_output[n]=xn_pred\n",
        "        rn=reservoirmap(rn,xn_pred,A,Win,leakage)    \n",
        "    return predicted_output,rn\n",
        "\n",
        "predicted_output,rn=reservoir_prediction(rn,Win,A,Wout,leakage,testing_length)\n",
        "plot_len = 500\n",
        "error = LA.norm(predicted_output[:plot_len,:] - xnlistnoiseless[training_length+training_transient:][:plot_len,:])\n",
        "print( '*****Prediction STD is '+str(pl.std(predicted_output)))\n",
        "print('*****Prediction error is '+str(error))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ohLDoIVyMG5U",
        "colab_type": "text"
      },
      "source": [
        "##Plot the results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PY8mgBODMJxV",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title\n",
        "pl.style.use('classic')\n",
        "\n",
        "pl.rc('font',family='cmr10')\n",
        "pl.rcParams['axes.unicode_minus']=False\n",
        "pl.rcParams.update({'figure.autolayout': True})\n",
        "\n",
        "pl.rcParams['font.size']=22.\n",
        "pl.rcParams['axes.labelsize']=22.\n",
        "pl.rcParams['legend.fontsize']=20.\n",
        "pl.rcParams['savefig.dpi']=200\n",
        "pl.rcParams['xtick.labelsize']=17.\n",
        "pl.rcParams['ytick.labelsize']=17.\n",
        "pl.rcParams['xtick.minor.size']=3\n",
        "pl.rcParams['xtick.major.size']=5\n",
        "pl.rcParams['ytick.minor.size']=3\n",
        "pl.rcParams['ytick.major.size']=5\n",
        "pl.rcParams['xtick.major.width']=1.\n",
        "pl.rcParams['xtick.minor.width']=1.\n",
        "pl.rcParams['ytick.major.width']=1.\n",
        "pl.rcParams['ytick.minor.width']=1.\n",
        "pl.rcParams['axes.linewidth']=2.\n",
        "\n",
        "## Use if you want to plot your data like an image\n",
        "#pl.subplot(311)\n",
        "#pl.imshow(predicted_output[:plot_len,:].T,interpolation='none',aspect='auto',vmin=-2.5,vmax=2.5,cmap='jet')\n",
        "#pl.title('Prediction');\n",
        "#pl.ylabel('hall probes');\n",
        "#pl.colorbar();\n",
        "#pl.subplot(312)\n",
        "#pl.imshow(xnlistnoiseless[training_length:][:plot_len,:].T,interpolation='none',aspect='auto',vmin=-2.5,vmax=2.5,cmap='jet')\n",
        "#pl.title('Truth');\n",
        "#pl.ylabel('hall probes');\n",
        "#pl.colorbar();\n",
        "#pl.subplot(313);\n",
        "#pl.imshow(predicted_output[:plot_len,:].T - xnlistnoiseless[training_length:][:plot_len,:].T,interpolation='none',aspect='auto',vmin=-2.5,vmax=2.5,cmap='jet')\n",
        "#pl.title('Error');\n",
        "#pl.ylabel('hall probes');\n",
        "#pl.xlabel('time (s)');\n",
        "#pl.colorbar();\n",
        "##pl.savefig('lstm_error.png')\n",
        "#pl.show()\n",
        "\n",
        "\n",
        "fig, axs = plt.subplots(3)\n",
        "axs[0].plot(timsh,predicted_output[:plot_len,0],label='prediction')\n",
        "axs[0].plot(t[training_length+training_transient:][:plot_len],xnlistnoiseless[training_length+training_transient:][:plot_len,0],label='truth')\n",
        "axs[1].plot(t[training_length+training_transient:][:plot_len],predicted_output[:plot_len,1],label='prediction');\n",
        "axs[1].plot(t[training_length+training_transient:][:plot_len],xnlistnoiseless[training_length+training_transient:][:plot_len,1],label='truth');\n",
        "axs[2].plot(t[training_length+training_transient:][:plot_len],predicted_output[:plot_len,2],label='prediction')\n",
        "axs[2].plot(t[training_length+training_transient:][:plot_len],xnlistnoiseless[training_length+training_transient:][:plot_len,2],label='truth');\n",
        "\n",
        "pl.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PqPkEew5tzUK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig = plt.figure()\n",
        "plot_len=150\n",
        "ax = fig.gca(projection='3d')\n",
        "ax.plot(xnlistnoiseless[training_length+training_transient:][:plot_len,0], xnlistnoiseless[training_length+training_transient:][:plot_len,1], xnlistnoiseless[training_length+training_transient:][:plot_len,2],label='truth')\n",
        "ax.plot(predicted_output[:plot_len,0], predicted_output[:plot_len,1], predicted_output[:plot_len,2])\n",
        "plt.draw()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}